import os
import pickle
import json
import re
from datetime import datetime
from dotenv import load_dotenv

# --- LangChain ë° RAG êµ¬ì„±ìš”ì†Œ ---
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field # LLM ì¶œë ¥ì„ êµ¬ì¡°í™”

# --- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬ì„±ìš”ì†Œ ---
from rank_bm25 import BM25Okapi
from tqdm import tqdm

TODAY_STR = datetime.now().strftime('%Y-%m-%d') 

CHROMA_DB_PATH = './skku_notice_db'
BM25_MODEL_PATH = 'bm25_model.pkl'
CHUNK_DATA_PATH = 'all_chunks.pkl'

EMBEDDING_MODEL = 'BAAI/bge-base-ko-v1.5'
COLLECTION_NAME = 'skku_notices'
LLM_MODEL = "gemini-1.5-flash-latest" 

VECTOR_SEARCH_K = 50  
BM25_SEARCH_K = 50   
FINAL_TOP_K = 5       
load_dotenv()
llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0)
print(f"âœ… [0ë‹¨ê³„] LLM ({LLM_MODEL}) ë¡œë“œ ì™„ë£Œ.")



class QueryFilter(BaseModel):
    """LLMì´ ì‚¬ìš©ìì˜ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ë°˜í™˜í•  êµ¬ì¡°"""
    core_query: str = Field(description="ê²€ìƒ‰ ì—”ì§„(BM25, Vector)ì— ì‚¬ìš©í•  í•µì‹¬ ê²€ìƒ‰ì–´")
    start_date: str = Field(description="í•„í„°ë§í•  ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD). ì—†ìœ¼ë©´ 'N/A'.")
    end_date: str = Field(description="í•„í„°ë§í•  ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD). ì—†ìœ¼ë©´ 'N/A'.")

def get_query_filter(llm: ChatGoogleGenerativeAI, user_query: str, today: str) -> QueryFilter:
    print(f"\nğŸ›ï¸ [1ë‹¨ê³„] ì¿¼ë¦¬ ë³€í™˜ ì‹œì‘ (ê¸°ì¤€ì¼: {today})...")
    print(f"  - ì›ë³¸ ì¿¼ë¦¬: \"{user_query}\"")

    structured_llm = llm.with_structured_output(QueryFilter)

    prompt = ChatPromptTemplate.from_messages([
        ("system", f"""
ë‹¹ì‹ ì€ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ í•„í„°ë¡œ ë³€í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤.
ì˜¤ëŠ˜ ë‚ ì§œëŠ” {today}ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1. 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì´ë²ˆ ì£¼' ê°™ì€ ìƒëŒ€ì  ë‚ ì§œë¥¼ {today} ê¸°ì¤€ìœ¼ë¡œ 'YYYY-MM-DD' í˜•ì‹ì˜ ì ˆëŒ€ ë‚ ì§œë¡œ ë³€í™˜í•˜ì„¸ìš”.
2. ë‚ ì§œ ë²”ìœ„ê°€ ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ start_dateì™€ end_date ëª¨ë‘ "N/A"ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
3. 'ì˜¤ëŠ˜ê¹Œì§€' = start_date: "N/A", end_date: {today}
4. 'ì˜¤ëŠ˜ë¶€í„°' = start_date: {today}, end_date: "N/A"
5. 'ì˜¤ëŠ˜ í•˜ëŠ”' = start_date: {today}, end_date: {today}
6. ë‚ ì§œì™€ ë¬´ê´€í•œ í‚¤ì›Œë“œëŠ” 'core_query'ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.

[ì¿¼ë¦¬ ì˜ˆì‹œ]
"ì–´ì œ ëë‚œ í–‰ì‚¬ ì•Œë ¤ì¤˜" (ì˜¤ëŠ˜ì´ 2025-11-17ì´ë©´)
{{ "core_query": "í–‰ì‚¬", "start_date": "N/A", "end_date": "2025-11-16" }}

"ì‹ ì²­í•  ìˆ˜ ìˆëŠ” ì¥í•™ê¸ˆ"
{{ "core_query": "ì‹ ì²­ ì¥í•™ê¸ˆ", "start_date": "N/A", "end_date": "N/A" }}
"""),
        ("human", f"ì¿¼ë¦¬: \"{user_query}\"")
    ])

    query_transformer_chain = prompt | structured_llm

    try:
        filter_obj = query_transformer_chain.invoke({"user_query": user_query})
        print(f"  - ë³€í™˜ ì™„ë£Œ: [Query: \"{filter_obj.core_query}\", Start: {filter_obj.start_date}, End: {filter_obj.end_date}]")
        return filter_obj
    except Exception as e:
        print(f"  - ì¿¼ë¦¬ ë³€í™˜ ì‹¤íŒ¨: {e}. ê¸°ë³¸ê°’ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        return QueryFilter(core_query=user_query, start_date="N/A", end_date="N/A")


def load_indexes():
    """ë””ìŠ¤í¬ì—ì„œ BM25, ChromaDB, all_chunksë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("\nğŸ›ï¸ [2ë‹¨ê³„] ë¡œì»¬ ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
    
    try:
        with open(BM25_MODEL_PATH, 'rb') as f:
            bm25 = pickle.load(f)
        print(f"  - BM25 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({BM25_MODEL_PATH})")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: BM25 ëª¨ë¸ íŒŒì¼({BM25_MODEL_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None, None, None

    try:
        with open(CHUNK_DATA_PATH, 'rb') as f:
            all_chunks = pickle.load(f)
        all_chunks_map = {chunk['id']: chunk for chunk in all_chunks}
        print(f"  - ì›ë³¸ ì²­í¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(all_chunks)}ê°œ ì²­í¬)")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: ì²­í¬ ë°ì´í„° íŒŒì¼({CHUNK_DATA_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None, None, None

    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        chroma_collection = Chroma(
            persist_directory=CHROMA_DB_PATH,
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME
        ).as_retriever(search_kwargs={"k": VECTOR_SEARCH_K})
        print(f"  - ChromaDB Vector Store ë¡œë“œ ì™„ë£Œ ({CHROMA_DB_PATH})")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: ChromaDB ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None, None

    return bm25, all_chunks, all_chunks_map, chroma_collection


def is_date_in_range(chunk_meta: dict, start_filter: str, end_filter: str) -> bool:
    """
    'N/A'ë¥¼ ê³ ë ¤í•˜ì—¬ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„°ê°€ ë‚ ì§œ ë²”ìœ„ í•„í„°ë¥¼ í†µê³¼í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    chunk_start = chunk_meta.get('start_date', 'N/A')
    chunk_end = chunk_meta.get('end_date', 'N/A')

    if start_filter != 'N/A':
        if chunk_end != 'N/A' and chunk_end < start_filter:
            return False

    if end_filter != 'N/A':
        if chunk_start != 'N/A' and chunk_start > end_filter:
            return False

    return True


def hybrid_search(query_filter: QueryFilter, collection, bm25, all_chunks, all_chunks_map):
    print("\nğŸ›ï¸ [3ë‹¨ê³„] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë° í•„í„°ë§ ì‹œì‘...")
    
    start_f = query_filter.start_date
    end_f = query_filter.end_date
    core_query = query_filter.core_query

    print(f"  - (A) Vector DB ê²€ìƒ‰ (K={VECTOR_SEARCH_K})...")
    vector_results = collection.invoke(core_query) 
    
    vector_filtered = {}
    for doc in vector_results:
        chunk_id = doc.metadata.get('id', doc.metadata.get('doc_id'))
        if not chunk_id: Warning("Chunk ID not found in metadata")
        
        if is_date_in_range(doc.metadata, start_f, end_f):
            vector_filtered[chunk_id] = doc.metadata.get('_score', 1.0)

    print(f"    - Vector ê²°ê³¼: {len(vector_results)}ê°œ ì¤‘ {len(vector_filtered)}ê°œ í•„í„° í†µê³¼.")

    print(f"  - (B) BM25 ê²€ìƒ‰ (ì „ì²´ {len(all_chunks)}ê°œ ìŠ¤ìº”)...")
    tokenized_query = core_query.split()
    bm25_scores = bm25.get_scores(tokenized_query)
    
    bm25_filtered = {}
    for score, chunk in zip(bm25_scores, all_chunks):
        if score > 0:
            if is_date_in_range(chunk['metadata'], start_f, end_f):
                bm25_filtered[chunk['id']] = score
    
    print(f"    - BM25 ê²°ê³¼: {len(all_chunks)}ê°œ ì¤‘ {len(bm25_filtered)}ê°œ í•„í„° í†µê³¼ (ì ìˆ˜ > 0).")

    print(f"  - (C) RRF ìœµí•© ì¤‘...")
    
    vec_ranked = sorted(vector_filtered.items(), key=lambda item: item[1], reverse=True)
    bm25_ranked = sorted(bm25_filtered.items(), key=lambda item: item[1], reverse=True)

    rrf_scores = {}
    k = 60 

    for rank, (chunk_id, score) in enumerate(vec_ranked):
        if chunk_id not in rrf_scores:
            rrf_scores[chunk_id] = 0
        rrf_scores[chunk_id] += 1 / (k + rank)

    for rank, (chunk_id, score) in enumerate(bm25_ranked):
        if chunk_id not in rrf_scores:
            rrf_scores[chunk_id] = 0
        rrf_scores[chunk_id] += 1 / (k + rank)

    fused_results = sorted(rrf_scores.items(), key=lambda item: item[1], reverse=True)
    
    print(f"  - ìœµí•© ì™„ë£Œ: ìµœì¢… {len(fused_results)}ê°œ ì²­í¬ í›„ë³´.")
    
    final_chunks_text = []
    for chunk_id, score in fused_results[:FINAL_TOP_K]:
        if chunk_id in all_chunks_map:
            chunk = all_chunks_map[chunk_id]
            chunk_text = f"--- [ì¶œì²˜: {chunk['metadata']['title']}]\n"
            chunk_text += f"{chunk['text']}\n"
            final_chunks_text.append(chunk_text)
        
    return "\n\n".join(final_chunks_text)


def generate_answer(llm: ChatGoogleGenerativeAI, context: str, user_query: str):
    print("\nğŸ›ï¸ [4ë‹¨ê³„] RAG ë‹µë³€ ìƒì„± ì‹œì‘...")

    if not context:
        print("  - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. LLMì´ ìì²´ì ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.")
        context = "ê²€ìƒ‰ëœ ê´€ë ¨ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤."

    prompt = ChatPromptTemplate.from_template(f"""
ë‹¹ì‹ ì€ ì„±ê· ê´€ëŒ€í•™êµ ê³µì§€ì‚¬í•­ ì•ˆë‚´ AIì…ë‹ˆë‹¤.
ì œê³µëœ [ê³µì§€ì‚¬í•­ ì»¨í…ìŠ¤íŠ¸]ë¥¼ ë°”íƒ•ìœ¼ë¡œ [ì‚¬ìš©ì ì¿¼ë¦¬]ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ì§€ì¹¨:
1. ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
2. ì»¨í…ìŠ¤íŠ¸ì— ë‚´ìš©ì´ ì—†ìœ¼ë©´ "ê´€ë ¨ ê³µì§€ì‚¬í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."ë¼ê³  ì†”ì§í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
3. ë‹µë³€ ì‹œ, ê·¼ê±°ê°€ ëœ ê³µì§€ì‚¬í•­ì˜ [ì¶œì²˜: ì œëª©]ì„ ëª…í™•íˆ ì–¸ê¸‰í•´ì•¼ í•©ë‹ˆë‹¤.
4. ë‹µë³€ ë§ˆì§€ë§‰ì—, ê´€ë ¨ ê³µì§€ì‚¬í•­ì˜ ì›ë³¸ ë§í¬(ì»¨í…ìŠ¤íŠ¸ì˜ 'URL')ë¥¼ "ìì„¸íˆ ë³´ê¸°" ëª©ë¡ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.

---
[ê³µì§€ì‚¬í•­ ì»¨í…ìŠ¤íŠ¸]
{context}
---
[ì‚¬ìš©ì ì¿¼ë¦¬]
{user_query}
---

[AI ë‹µë³€]
""")

    rag_chain = prompt | llm | StrOutputParser()
    
    response = rag_chain.invoke({"context": context, "user_query": user_query})
    
    print("âœ… [5ë‹¨ê³„] ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    return response


if __name__ == "__main__":
    
    USER_QUERY = "ì˜¤ëŠ˜ ì‹ ì²­í•  ìˆ˜ ìˆëŠ” ìœµí•©ì—°êµ¬í•™ì ì œ ê³µì§€ ìˆì–´?"
    
    print("="*50)
    print(f"RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ê¸°ì¤€ì¼: {TODAY_STR})")
    print(f"ì‚¬ìš©ì ì¿¼ë¦¬: \"{USER_QUERY}\"")
    print("="*50)

    bm25_model, all_chunks_list, chunks_map, chroma_retriever = load_indexes()
    
    if bm25_model:
        query_filter = get_query_filter(llm, USER_QUERY, TODAY_STR)
        
        context_string = hybrid_search(
            query_filter, 
            chroma_retriever, 
            bm25_model, 
            all_chunks_list,
            chunks_map
        )
        
        final_response = generate_answer(llm, context_string, USER_QUERY)
        
        print("\n" + "="*50)
        print("[ìµœì¢… ìƒì„± ë‹µë³€]")
        print("="*50)
        print(final_response)
    else:
        print("ì˜¤ë¥˜: ì¸ë±ìŠ¤ íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € build_index.pyë¥¼ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")