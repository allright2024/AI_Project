import os
import pickle
import json
import re
from datetime import datetime
from dotenv import load_dotenv

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from pydantic import BaseModel, Field

from rank_bm25 import BM25Okapi
import numpy as np

load_dotenv()

TODAY_STR = datetime.now().strftime('%Y-%m-%d')

CHROMA_DB_PATH = './skku_notice_db'
BM25_MODEL_PATH = 'bm25_model.pkl'
CHUNK_DATA_PATH = 'all_chunks.pkl'

EMBEDDING_MODEL = "jhgan/ko-sbert-nli"
COLLECTION_NAME = 'skku_notices'
LLM_MODEL = "gemini-2.5-flash-lite"

VECTOR_SEARCH_K = 50
BM25_SEARCH_K = 50
FINAL_TOP_K = 5

llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0)
print(f"âœ… [0ë‹¨ê³„] LLM ({LLM_MODEL}) ë¡œë“œ ì™„ë£Œ.")


class QueryFilter(BaseModel):
    """LLMì´ ì‚¬ìš©ìì˜ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ë°˜í™˜í•  êµ¬ì¡°"""
    core_query: str = Field(description="ê²€ìƒ‰ ì—”ì§„(BM25, Vector)ì— ì‚¬ìš©í•  í•µì‹¬ ê²€ìƒ‰ì–´")
    start_date: str = Field(description="í•„í„°ë§í•  ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD). ì—†ìœ¼ë©´ 'N/A'.")
    end_date: str = Field(description="í•„í„°ë§í•  ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD). ì—†ìœ¼ë©´ 'N/A'.")

def get_query_filter(llm: ChatGoogleGenerativeAI, user_query: str, today: str) -> QueryFilter:
    print(f"\nğŸ›ï¸ [1ë‹¨ê³„] ì¿¼ë¦¬ ë³€í™˜ ì‹œì‘ (ê¸°ì¤€ì¼: {today})...")
    
    structured_llm = llm.with_structured_output(QueryFilter)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """
ë‹¹ì‹ ì€ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ í•„í„°ë¡œ ë³€í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤.
ì˜¤ëŠ˜ ë‚ ì§œëŠ” {today}ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1. 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì´ë²ˆ ì£¼' ê°™ì€ ìƒëŒ€ì  ë‚ ì§œë¥¼ {today} ê¸°ì¤€ìœ¼ë¡œ 'YYYY-MM-DD' í˜•ì‹ì˜ ì ˆëŒ€ ë‚ ì§œë¡œ ë³€í™˜í•˜ì„¸ìš”.
2. ë‚ ì§œ ë²”ìœ„ê°€ ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ start_dateì™€ end_date ëª¨ë‘ "N/A"ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
3. 'ì˜¤ëŠ˜ê¹Œì§€' = start_date: "N/A", end_date: {today}
4. 'ì˜¤ëŠ˜ë¶€í„°' = start_date: {today}, end_date: "N/A"
5. 'ì˜¤ëŠ˜ í•˜ëŠ”' = start_date: {today}, end_date: {today}
6. ë‚ ì§œì™€ ë¬´ê´€í•œ í‚¤ì›Œë“œëŠ” 'core_query'ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.

[ì¿¼ë¦¬ ì˜ˆì‹œ]
"ì–´ì œ ëë‚œ í–‰ì‚¬ ì•Œë ¤ì¤˜" (ì˜¤ëŠ˜ì´ 2025-11-17ì´ë©´)
-> {{ "core_query": "í–‰ì‚¬", "start_date": "N/A", "end_date": "2025-11-16" }}

"ì‹ ì²­í•  ìˆ˜ ìˆëŠ” ì¥í•™ê¸ˆ"
-> {{ "core_query": "ì‹ ì²­ ì¥í•™ê¸ˆ", "start_date": "N/A", "end_date": "N/A" }}
"""),
        ("human", "ì¿¼ë¦¬: \"{user_query}\"")
    ])
    
    query_transformer_chain = prompt | structured_llm
    
    try:
        filter_obj = query_transformer_chain.invoke({
            "user_query": user_query, 
            "today": today
        })
        print(f"  - ë³€í™˜ ì™„ë£Œ: [Query: \"{filter_obj.core_query}\", Start: {filter_obj.start_date}, End: {filter_obj.end_date}]")
        return filter_obj
    except Exception as e:
        print(f"  - ì¿¼ë¦¬ ë³€í™˜ ì‹¤íŒ¨: {e}. ê¸°ë³¸ê°’ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        return QueryFilter(core_query=user_query, start_date="N/A", end_date="N/A")


def load_indexes():
    """ë””ìŠ¤í¬ì—ì„œ BM25, ChromaDB, all_chunksë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("\nğŸ›ï¸ [2ë‹¨ê³„] ë¡œì»¬ ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
    
    try:
        with open(BM25_MODEL_PATH, 'rb') as f:
            bm25 = pickle.load(f)
        print(f"  - BM25 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({BM25_MODEL_PATH})")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: BM25 ëª¨ë¸ íŒŒì¼({BM25_MODEL_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None, None, None, None

    try:
        with open(CHUNK_DATA_PATH, 'rb') as f:
            all_chunks = pickle.load(f)
        all_chunks_map = {chunk['id']: chunk for chunk in all_chunks}
        print(f"  - ì›ë³¸ ì²­í¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(all_chunks)}ê°œ ì²­í¬)")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: ì²­í¬ ë°ì´í„° íŒŒì¼({CHUNK_DATA_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None, None, None, None

    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        chroma_collection = Chroma(
            persist_directory=CHROMA_DB_PATH,
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME
        )
        print(f"  - ChromaDB Vector Store ë¡œë“œ ì™„ë£Œ ({CHROMA_DB_PATH})")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: ChromaDB ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None, None, None

    return bm25, all_chunks, all_chunks_map, chroma_collection


def build_chroma_filter(start_f: str, end_f: str) -> dict:
    """
    ChromaDB ê²€ìƒ‰ìš© ë©”íƒ€ë°ì´í„° í•„í„°(Where ì ˆ)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (Pre-Filteringìš©)
    """
    conditions = []

    if start_f != "N/A":
        conditions.append({
            "$or": [
                {"end_date": {"$gte": start_f}},
                {"end_date": {"$eq": "N/A"}}
            ]
        })

    if end_f != "N/A":
        conditions.append({
            "$or": [
                {"start_date": {"$lte": end_f}},
                {"start_date": {"$eq": "N/A"}}
            ]
        })

    if not conditions:
        return None
    
    if len(conditions) == 1:
        return conditions[0]

    return {"$and": conditions}


def is_date_in_range(chunk_meta: dict, start_filter: str, end_filter: str) -> bool:
    """
    BM25 ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ì„ ìœ„í•œ Python í•¨ìˆ˜ (Post-Filteringìš©)
    """
    chunk_start = chunk_meta.get('start_date', 'N/A')
    chunk_end = chunk_meta.get('end_date', 'N/A')

    if start_filter != 'N/A':
        if chunk_end != 'N/A' and chunk_end < start_filter:
            return False

    if end_filter != 'N/A':
        if chunk_start != 'N/A' and chunk_start > end_filter:
            return False

    return True


def hybrid_search(query_filter: QueryFilter, collection, bm25, all_chunks, all_chunks_map):
    print("\nğŸ›ï¸ [3ë‹¨ê³„] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘ (Pre-Filtering ì ìš©)...")
    
    start_f = query_filter.start_date
    end_f = query_filter.end_date
    core_query = query_filter.core_query

    chroma_filter = build_chroma_filter(start_f, end_f)
    print(f"  - Chroma Filter ì¡°ê±´: {chroma_filter}")

    vector_results_with_score = collection.similarity_search_with_score(
        query=core_query,
        k=VECTOR_SEARCH_K, 
        filter=chroma_filter 
    )

    print(f"    - Vector(Pre-filtered) ê²°ê³¼: {len(vector_results_with_score)}ê°œ ë°˜í™˜ë¨.")

    print(f"  - BM25 ê²€ìƒ‰ (ì „ì²´ {len(all_chunks)}ê°œ ìŠ¤ìº”)...")
    tokenized_query = core_query.split()
    bm25_scores = bm25.get_scores(tokenized_query)
    
    top_n_indices = np.argsort(bm25_scores)[::-1][:BM25_SEARCH_K * 3]

    bm25_filtered = {}
    for idx in top_n_indices:
        score = bm25_scores[idx]
        if score <= 0: continue
        
        chunk = all_chunks[idx]
        # ë‚ ì§œ í•„í„°ë§ ìˆ˜í–‰
        if is_date_in_range(chunk['metadata'], start_f, end_f):
            bm25_filtered[chunk['id']] = score

    print(f"    - BM25(Post-filtered) ê²°ê³¼: {len(bm25_filtered)}ê°œ í†µê³¼.")

    print(f"  - (C) RRF ìœµí•© ì¤‘...")
    
    rrf_scores = {}
    k = 60 

    for rank, (doc, score) in enumerate(vector_results_with_score):
        chunk_id = doc.metadata.get('id')
        if chunk_id:
            if chunk_id not in rrf_scores: rrf_scores[chunk_id] = 0
            rrf_scores[chunk_id] += 1 / (k + rank)

    bm25_sorted = sorted(bm25_filtered.items(), key=lambda item: item[1], reverse=True)
    for rank, (chunk_id, score) in enumerate(bm25_sorted):
        if chunk_id not in rrf_scores: rrf_scores[chunk_id] = 0
        rrf_scores[chunk_id] += 1 / (k + rank)

    fused_results = sorted(rrf_scores.items(), key=lambda item: item[1], reverse=True)
    print(f"  - ìœµí•© ì™„ë£Œ: ìµœì¢… {len(fused_results)}ê°œ ì²­í¬ í›„ë³´.")
    
    final_chunks_text = []
    for chunk_id, score in fused_results[:FINAL_TOP_K]:
        if chunk_id in all_chunks_map:
            chunk = all_chunks_map[chunk_id]
            url = chunk['metadata'].get('url', '')
            
            chunk_text = f"--- [ì¶œì²˜: {chunk['metadata']['title']}] ---\n"
            if url: chunk_text += f"[ì›ë³¸ ë§í¬: {url}]\n"
            chunk_text += f"{chunk['text']}\n"
            final_chunks_text.append(chunk_text)
        
    return "\n\n".join(final_chunks_text)


def generate_answer(llm: ChatGoogleGenerativeAI, context: str, user_query: str):
    print("\nğŸ›ï¸ [4ë‹¨ê³„] RAG ë‹µë³€ ìƒì„± ì‹œì‘...")

    if not context:
        print("  - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        context = "ê²€ìƒ‰ëœ ê´€ë ¨ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤."

    prompt = ChatPromptTemplate.from_template(f"""
ë‹¹ì‹ ì€ ì„±ê· ê´€ëŒ€í•™êµ ê³µì§€ì‚¬í•­ ì•ˆë‚´ AIì…ë‹ˆë‹¤.
ì œê³µëœ [ê³µì§€ì‚¬í•­ ì»¨í…ìŠ¤íŠ¸]ë¥¼ ë°”íƒ•ìœ¼ë¡œ [ì‚¬ìš©ì ì¿¼ë¦¬]ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ì§€ì¹¨:
1. ë°˜ë“œì‹œ ì œê³µëœ [ê³µì§€ì‚¬í•­ ì»¨í…ìŠ¤íŠ¸]ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³  "ê´€ë ¨ ê³µì§€ì‚¬í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì„¸ìš”.
3. ë‹µë³€ ì‹œ ì¶œì²˜(ì œëª©)ë¥¼ ì–¸ê¸‰í•˜ê³ , ì œê³µëœ 'ì›ë³¸ ë§í¬'ê°€ ìˆë‹¤ë©´ ë‹µë³€ ëì— ëª©ë¡ìœ¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.
4. ì‚¬ìš©ì ì§ˆë¬¸ì´ 'ì˜¤ëŠ˜ ê°€ëŠ¥í•œ' ê²ƒì„ ë¬»ëŠ”ë‹¤ë©´, ë‚ ì§œë¥¼ ê¼¼ê¼¼íˆ í™•ì¸í•˜ì„¸ìš”.

---
[ê³µì§€ì‚¬í•­ ì»¨í…ìŠ¤íŠ¸]
{context}
---
[ì‚¬ìš©ì ì¿¼ë¦¬]
{user_query}
---

[AI ë‹µë³€]
""")

    rag_chain = prompt | llm | StrOutputParser()
    response = rag_chain.invoke({"context": context, "user_query": user_query})
    print("âœ… [5ë‹¨ê³„] ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    return response


if __name__ == "__main__":
    USER_QUERY = "ì˜¤ëŠ˜ ì‹ ì²­í•  ìˆ˜ ìˆëŠ” ìœµí•©ì—°êµ¬í•™ì ì œ ê³µì§€ ìˆì–´?"
    
    print("="*60)
    print(f"RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ê¸°ì¤€ì¼: {TODAY_STR})")
    print(f"ì‚¬ìš©ì ì¿¼ë¦¬: \"{USER_QUERY}\"")
    print("="*60)

    # ì¸ë±ìŠ¤ ë¡œë“œ
    bm25_model, all_chunks_list, chunks_map, chroma_collection = load_indexes()
    
    if bm25_model and chroma_collection:
        query_filter = get_query_filter(llm, USER_QUERY, TODAY_STR)
        
        context_string = hybrid_search(
            query_filter, 
            chroma_collection, 
            bm25_model, 
            all_chunks_list,
            chunks_map
        )
        
        final_response = generate_answer(llm, context_string, USER_QUERY)
        
        print("\n" + "="*60)
        print("[ìµœì¢… ìƒì„± ë‹µë³€]")
        print("="*60)
        print(final_response)
    else:
        print("ì˜¤ë¥˜: ì¸ë±ìŠ¤ íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
