import json
import os
import pickle  # BM25 ëª¨ë¸ ì €ì¥
from langchain_text_splitters import RecursiveCharacterTextSplitter
import chromadb
from chromadb.utils import embedding_functions
from rank_bm25 import BM25Okapi
from tqdm import tqdm 

AUGMENTED_JSON_PATH = 'skku_augmented.json' # 2_LLM_using_extract_day.py ê±°ì¹œ íŒŒì¼

# ì¶œë ¥ íŒŒì¼ ë° ë””ë ‰í„°ë¦¬
CHROMA_DB_PATH = './skku_notice_db'  # Vector DB ì €ì¥
BM25_MODEL_PATH = 'bm25_model.pkl'   # BM25 ëª¨ë¸ íŒŒì¼
CHUNK_DATA_PATH = 'all_chunks.pkl'   # BM25 ë§¤í•‘ì„ ìœ„í•œ ì›ë³¸ ì²­í¬ ë°ì´í„°

# ëª¨ë¸ ë° ì„¤ì •
COLLECTION_NAME = 'skku_notices'
EMBEDDING_MODEL = "jhgan/ko-sbert-nli" # pretrained ëª¨ë¸
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50


def load_augmented_notices(file_path):
    """
    ì´ì „ ë‹¨ê³„ì—ì„œ LLMìœ¼ë¡œ 'start_date', 'end_date'ê°€
    ì¶”ê°€ëœ 'skku_augmented.json' íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            notices = json.load(f)
        print(f"âœ… [1ë‹¨ê³„] '{file_path}'ì—ì„œ {len(notices)}ê°œì˜ ê³µì§€ì‚¬í•­ ë¡œë“œ ì™„ë£Œ.")
        return notices
    except FileNotFoundError:
        print(f"âŒ [1ë‹¨ê³„] ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("           ë¨¼ì € LLMìœ¼ë¡œ ë‚ ì§œë¥¼ ì¶”ì¶œí•˜ê³  ì €ì¥í•˜ëŠ” ì½”ë“œë¥¼ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")
        return None
    except json.JSONDecodeError:
        print(f"âŒ [1ë‹¨ê³„] ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return None


# --- 3. í…ìŠ¤íŠ¸ ë¶„í•  (Chunking) ---
def run_chunking(augmented_notices):
    """
    ê³µì§€ì‚¬í•­ì„ ì˜ë¯¸ ë‹¨ìœ„ì˜ ì²­í¬(chunk)ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    (RecursiveCharacterTextSplitter, 500/50 ì„¤ì •)
    """
    print("\nğŸ›ï¸ [2ë‹¨ê³„] í…ìŠ¤íŠ¸ ë¶„í• (Chunking) ì‹œì‘...")
    
    # 1. ë¶„í• ê¸° ì •ì˜ (ì‚¬ìš©ìì™€ í•©ì˜í•œ ì„¤ì •)
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", " ", ""], # ë¬¸ë‹¨ -> ì¤„ë°”ê¿ˆ -> ë„ì–´ì“°ê¸° ìˆœ
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        is_separator_regex=False,
    )
    
    all_chunks = []
    
    for doc_id, notice in enumerate(tqdm(augmented_notices, desc="  Chunking")):
        text_to_split = notice.get('content', '')
        
        # 2. ê° ì²­í¬ê°€ ìƒì†ë°›ì„ ë©”íƒ€ë°ì´í„°
        # (contentëŠ” ë¶„í• ë˜ë¯€ë¡œ ì œì™¸)
        metadata = {
            "url": notice.get("url"),
            "title": notice.get("title"),
            "start_date": notice.get("start_date", "N/A"),
            "end_date": notice.get("end_date", "N/A"),
            # "category": notice.get("category", "ì¼ë°˜"), # (ì¹´í…Œê³ ë¦¬ë„ ì¶”ì¶œí–ˆë‹¤ë©´ í¬í•¨)
        }
        
        # 3. í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤í–‰
        chunks_texts = text_splitter.split_text(text_to_split)
        
        # 4. ë¶„í• ëœ ì²­í¬ + ë©”íƒ€ë°ì´í„° ì €ì¥
        for chunk_index, chunk_text in enumerate(chunks_texts):
            chunk_data = {
                "id": f"doc_{doc_id}_chunk_{chunk_index}", # ê³ ìœ  ID
                "text": chunk_text,                      # ë¶„í• ëœ í…ìŠ¤íŠ¸
                "metadata": metadata                     # ì›ë³¸ ë¬¸ì„œ ì •ë³´
            }
            all_chunks.append(chunk_data)

    print(f"âœ… [2ë‹¨ê³„] {len(augmented_notices)}ê°œ ë¬¸ì„œë¥¼ {len(all_chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ.")
    return all_chunks


# --- 4. í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ì‹± ë° ì €ì¥ ---
def run_hybrid_indexing(all_chunks):
    """
    ë¶„í• ëœ ì²­í¬ë¥¼ Vector DB(Chroma)ì™€ BM25ì— ì¸ë±ì‹±í•˜ê³  *íŒŒì¼ë¡œ ì €ì¥*í•©ë‹ˆë‹¤.
    """
    print("\nğŸ›ï¸ [3ë‹¨ê³„] í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ì‹± ë° ì €ì¥ ì‹œì‘...")

    # --- 4-1. Vector Index (ChromaDB) ---
    print("  [3-1] Vector DB (ChromaDB) ì¸ë±ì‹± ì¤‘...")
    
    # 1. ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì˜êµ¬ ì €ì¥)
    if not os.path.exists(CHROMA_DB_PATH):
        os.makedirs(CHROMA_DB_PATH)
    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

    # 2. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL,
        device='cpu' # GPU ì‚¬ìš© ì‹œ 'cuda'
    )

    # 3. ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
    collection = client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=sentence_transformer_ef,
        metadata={"hnsw:space": "cosine"} # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    )

    # 4. ChromaDBì— ì €ì¥í•  í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì¤€ë¹„
    documents = [chunk['text'] for chunk in all_chunks]
    metadatas = [chunk['metadata'] for chunk in all_chunks]
    ids = [chunk['id'] for chunk in all_chunks]

    # 5. DBì— ë°ì´í„° ì¶”ê°€ (Upsert: ë®ì–´ì“°ê¸°)
    # (ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•˜ì—¬ ë©”ëª¨ë¦¬ ê´€ë¦¬)
    batch_size = 5000
    for i in tqdm(range(0, len(ids), batch_size), desc="  Chroma Upsert"):
        collection.upsert(
            ids=ids[i:i + batch_size],
            documents=documents[i:i + batch_size],
            metadatas=metadatas[i:i + batch_size]
        )
    
    print(f"  âœ… [3-1] Vector DB ì¸ë±ì‹± ì™„ë£Œ. '{CHROMA_DB_PATH}'ì— ì €ì¥ë¨.")

    # --- 4-2. Keyword Index (BM25) ---
    print("  [3-2] BM25 ì¸ë±ì‹± ì¤‘...")

    # 1. BM25ìš© í† í°í™”ëœ ì½”í¼ìŠ¤ ìƒì„±
    # (ì£¼ì˜: ê°„ë‹¨í•œ ê³µë°± ê¸°ì¤€. ì„±ëŠ¥ì„ ë†’ì´ë ¤ë©´ MeCab, KoNLPy ë“± í˜•íƒœì†Œ ë¶„ì„ê¸° ê¶Œì¥)
    print("    - BM25 ì½”í¼ìŠ¤ í† í°í™” ì¤‘...")
    tokenized_corpus = [chunk['text'].split() for chunk in tqdm(all_chunks)]

    # 2. BM25 ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ
    print("    - BM25 ëª¨ë¸ í•™ìŠµ ì¤‘...")
    bm25 = BM25Okapi(tokenized_corpus)

    # 3. BM25 ëª¨ë¸ê³¼ ì›ë³¸ ì²­í¬ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (pickle)
    with open(BM25_MODEL_PATH, 'wb') as f_bm25:
        pickle.dump(bm25, f_bm25)
        print(f"  âœ… [3-2] BM25 ëª¨ë¸ ì €ì¥ ì™„ë£Œ. ({BM25_MODEL_PATH})")
        
    with open(CHUNK_DATA_PATH, 'wb') as f_chunks:
        pickle.dump(all_chunks, f_chunks)
        print(f"  âœ… [3-2] BM25 ë§¤í•‘ìš© ì²­í¬ ë°ì´í„° ì €ì¥ ì™„ë£Œ. ({CHUNK_DATA_PATH})")
    
    print(f"âœ… [3ë‹¨ê³„] í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ì‹± ì™„ë£Œ.")


# --- 5. ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    
    # 1. LLMìœ¼ë¡œ ê°•í™”ëœ JSON ë¡œë“œ
    augmented_notices = load_augmented_notices(AUGMENTED_JSON_PATH)
    
    if augmented_notices:
        # 2. í…ìŠ¤íŠ¸ ë¶„í•  (Chunking)
        chunks = run_chunking(augmented_notices)
        
        # 3. í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ì‹± ë° ë””ìŠ¤í¬ ì €ì¥
        run_hybrid_indexing(chunks)
        
        print("\nğŸ‰ RAG ì „ì²˜ë¦¬(ì¸ë±ì‹±) íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"  - Vector DB: '{CHROMA_DB_PATH}'")
        print(f"  - BM25 Model: '{BM25_MODEL_PATH}'")
        print(f"  - Chunk Data: '{CHUNK_DATA_PATH}'")
        print("ì´ì œ ì´ íŒŒì¼ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ ì²˜ë¦¬(ì„œë¹„ìŠ¤) íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
